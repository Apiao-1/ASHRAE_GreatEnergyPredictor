{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "from meteocalc import feels_like, Temp\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "ope = os.path.exists\n",
    "osp = os.path\n",
    "opj = os.path.join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(\n",
    "\"\"\"\n",
    "work_dir: \"C:/Users/cakey/ashrae-energy-prediction\"\n",
    "data_dir: \"C:/Users/cakey/ashrae-energy-prediction/input\"\n",
    "\n",
    "seed : 4534\n",
    "workers: 5\n",
    "\n",
    "train:\n",
    "    folds: 3\n",
    "    model: lightgbm\n",
    "    learning_rate: 0.05\n",
    "    num_rounds: 1000000\n",
    "    early_stopping: 50\n",
    "    \n",
    "test:\n",
    "    batch_size: 1000000\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    if verbose: print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_building(file_dir=None):\n",
    "    building = pd.read_csv(file_dir)\n",
    "    building = reduce_mem_usage(building, use_float16=True)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    building[\"primary_use\"] = le.fit_transform(building[\"primary_use\"])\n",
    "\n",
    "    sq_m = building.loc[(building['site_id'].isin([1])) & (~building['building_id'].isin([150, 106])), 'square_feet'] \n",
    "    building.loc[(building['site_id'].isin([1])) & (~building['building_id'].isin([150, 106])), 'square_feet'] = sq_m * 10.7639\n",
    "    building['floor_count'] = building['floor_count']\n",
    "    building['year_built'] = building['year_built']\n",
    "    building[\"square_feet_floor\"] = building['square_feet']/building['floor_count']\n",
    "    building['square_feet_floor'] = building['square_feet_floor'].replace(np.inf, building['square_feet'])\n",
    "    building['square_feet'] =  np.log1p(building['square_feet'])\n",
    "    building['square_feet_floor'] = np.log1p(building['square_feet_floor'])\n",
    "        \n",
    "    return building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_weather(file_dir=None):\n",
    "    \n",
    "    weather = pd.read_csv(file_dir)\n",
    "    weather = impute_weather(weather)\n",
    "    weather = reduce_mem_usage(weather, use_float16=True)\n",
    "\n",
    "    # emwa...\n",
    "    weather['air_temperature_m3'] = weather['air_temperature'].shift(-3)\n",
    "    weather['air_temperature_m2'] = weather['air_temperature'].shift(-2)\n",
    "    weather['air_temperature_m1'] = weather['air_temperature'].shift(-1)\n",
    "    \n",
    "    return weather\n",
    "\n",
    "# Original code from https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling by @aitude\n",
    "def impute_weather(weather_df):\n",
    "    start_date = datetime.strptime(weather_df['timestamp'].min(), \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date = datetime.strptime(weather_df['timestamp'].max(), \"%Y-%m-%d %H:%M:%S\")\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - timedelta(hours=x)).strftime(\"%Y-%m-%d %H:%M:%S\") for x in range(total_hours)]\n",
    "    \n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df, new_rows], sort=True)\n",
    "        weather_df = weather_df.reset_index(drop=True)           \n",
    "    \n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n",
    "    \n",
    "    def get_meteorological_features(data):\n",
    "        def calculate_rh(df):\n",
    "            df['relative_humidity'] = 100 * (np.exp((17.625 * df['dew_temperature']) / (243.04 + df['dew_temperature'])) / np.exp((17.625 * df['air_temperature'])/(243.04 + df['air_temperature'])))\n",
    "        def calculate_fl(df):\n",
    "            flike_final = []\n",
    "            flike = []\n",
    "            for i in range(len(df)):\n",
    "                at = df['air_temperature'][i]\n",
    "                rh = df['relative_humidity'][i]\n",
    "                ws = df['wind_speed'][i]\n",
    "                flike.append(feels_like(Temp(at, unit = 'C'), rh, ws))\n",
    "            for i in range(len(flike)):\n",
    "                flike_final.append(flike[i].f)\n",
    "            df['feels_like'] = flike_final\n",
    "            del flike_final, flike, at, rh, ws\n",
    "        calculate_rh(data)\n",
    "        calculate_fl(data)\n",
    "        return data\n",
    "    weather_df = get_meteorological_features(weather_df)\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(df):\n",
    "    df.reset_index(drop=True)\n",
    "    df['timestamp'] = pd.to_datetime(arg=df['timestamp'] ,format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.weekday\n",
    "    df['year_day'] = df['timestamp'].dt.dayofyear\n",
    "    df.loc[df['day_of_week'].isin([1,2,3]), 'day_of_week'] = 1\n",
    "    \n",
    "    holidays = [\n",
    "        \"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "        \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "        \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "        \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "        \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "        \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "        \"2019-01-01\"]\n",
    "    df[\"is_holiday\"] = (df.timestamp.dt.date.isin(holidays)).astype(int)\n",
    "\n",
    "    df['group'] = df['timestamp'].dt.month\n",
    "    df['group'].replace((1, 2, 3, 4), 1, inplace = True)\n",
    "    df['group'].replace((5, 6, 7 ,8), 2, inplace = True)\n",
    "    df['group'].replace((9, 10, 11, 12), 3, inplace = True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train(meter=0):\n",
    "    train = pd.read_csv(opj(config[\"data_dir\"], \"train.csv\"))\n",
    "    train = reduce_mem_usage(train, use_float16=True)\n",
    "    building = data_building(file_dir=opj(config[\"data_dir\"], \"building_metadata.csv\"))\n",
    "    train = train.merge(building, left_on='building_id',right_on='building_id',how='left')\n",
    "    weather = data_weather(file_dir=opj(config[\"data_dir\"], \"weather_train.csv\"))\n",
    "    train = train.merge(weather, how='left', on=['site_id','timestamp'])\n",
    "    train = data(train)\n",
    "    \n",
    "    train['meter_reading'] = np.log1p(train[\"meter_reading\"])\n",
    "    \n",
    "    bad_rows = pd.read_csv(opj(config[\"data_dir\"], \"rows_to_drop.csv\"))\n",
    "    train = train.drop(bad_rows.loc[:, '0']).reset_index(drop = True)\n",
    "    \n",
    "    train = train.drop([\"timestamp\"], axis=1)\n",
    "    \n",
    "    train = train.loc[train['meter'] == meter].reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "        \n",
    "def create_test(meter=0):\n",
    "    test = pd.read_csv(opj(config[\"data_dir\"], \"test.csv\"))\n",
    "    test = reduce_mem_usage(test, use_float16=True)\n",
    "    building = data_building(file_dir=opj(config[\"data_dir\"], \"building_metadata.csv\"))\n",
    "    test = test.merge(building, left_on='building_id',right_on='building_id',how='left')\n",
    "    weather = data_weather(file_dir=opj(config[\"data_dir\"], \"weather_test.csv\"))\n",
    "    test = test.merge(weather, how='left', on=['site_id','timestamp'])\n",
    "    test = data(test)\n",
    "    test = test.drop([\"timestamp\"], axis=1)\n",
    "    \n",
    "    test = test.loc[test['meter'] == meter].reset_index(drop=True)\n",
    "    \n",
    "    return test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for m in range(4):       \n",
    "    if m==0:\n",
    "        {'num_leaves': 526, 'max_depth': 12, 'bagging_fraction': 0.10081820060774621, 'feature_fraction': 0.7742020727078566}\n",
    "    if m==1:\n",
    "        {'num_leaves': 261, 'max_depth': 10, 'bagging_fraction': 0.26016952708832514, 'feature_fraction': 0.5491805155524557}\n",
    "    if m==2:\n",
    "        {'num_leaves': 305, 'max_depth': 12, 'bagging_fraction': 0.9788344136664392, 'feature_fraction': 0.7869347797200678}\n",
    "    if m==3:\n",
    "        {'num_leaves': 917, 'max_depth': 11, 'bagging_fraction': 0.15036565575986183, 'feature_fraction': 0.3663760577105429}\n",
    "\n",
    "    params.update({\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': config['train']['learning_rate'],\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"metric\": 'rmse',   \n",
    "        'num_threads': config['workers'],\n",
    "        'seed': config['seed'],\n",
    "        \"verbosity\": -1,\n",
    "    })   \n",
    "    \n",
    "    train = create_train(meter=m)\n",
    "    test = create_test(meter=m)\n",
    "\n",
    "    oof_train = np.zeros((train.shape[0]))\n",
    "\n",
    "    models = []\n",
    "    kf = GroupKFold(n_splits=config['train']['folds'])\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(train, groups = train['group'])):\n",
    "\n",
    "        d_train = lgb.Dataset(\n",
    "            train.iloc[train_idx].drop(columns=['meter', 'meter_reading', 'group', \"month\"]),\n",
    "            label=train['meter_reading'].iloc[train_idx],\n",
    "            categorical_feature=None)\n",
    "\n",
    "        d_valid = lgb.Dataset(\n",
    "            train.iloc[valid_idx].drop(columns=['meter', 'meter_reading', 'group', \"month\"]), \n",
    "            label=train['meter_reading'].iloc[valid_idx], \n",
    "            categorical_feature=None)\n",
    "\n",
    "        watchlist = [d_train, d_valid]\n",
    "\n",
    "        categorical_feats = [train.drop(columns=['meter', 'meter_reading', 'group', \"month\"]).columns.get_loc(c) for c in ['building_id', 'site_id', 'primary_use', 'day_of_week', 'is_holiday'] ]\n",
    "\n",
    "        mdl = lgb.train(\n",
    "            params,\n",
    "            train_set=d_train,\n",
    "            categorical_feature=categorical_feats,\n",
    "            valid_sets=watchlist,\n",
    "            verbose_eval=False,\n",
    "            num_boost_round=config['train']['num_rounds'],\n",
    "            early_stopping_rounds=config['train']['early_stopping'],\n",
    "        )\n",
    "\n",
    "        # predict on out-of-fold samples...\n",
    "        y_valid_pred = mdl.predict(train.iloc[valid_idx].drop(columns=['meter', 'meter_reading', 'group', \"month\"]), num_iteration=mdl.best_iteration)\n",
    "        oof_train[valid_idx] += y_valid_pred\n",
    "\n",
    "        score = np.sqrt(mean_squared_error(train['meter_reading'].iloc[valid_idx], y_valid_pred))\n",
    "        print('Fold: %s \\t Validation: %s' % (fold, score))\n",
    "\n",
    "        models.append(mdl)\n",
    "\n",
    "        del mdl\n",
    "        gc.collect()\n",
    "\n",
    "    validation = pd.DataFrame({\n",
    "        'meter_reading': train['meter_reading'],\n",
    "        \"meter_reading_oof\": oof_train\n",
    "    })\n",
    "\n",
    "    save_file = opj(config[\"work_dir\"], \"results\", \"validation_model=%s_meter=%s.csv\" % (\n",
    "        config['train']['model'], m))\n",
    "    validation.to_csv(save_file, index=False)\n",
    "\n",
    "    score = np.sqrt(mean_squared_error(train['meter_reading'], oof_train))\n",
    "    print('Validation: %s' % (score))\n",
    "\n",
    "    del oof_train\n",
    "    gc.collect()\n",
    "    \n",
    "    def predictions(meter, test, models):\n",
    "\n",
    "        batch_size = int(config['test']['batch_size'])\n",
    "        iterations = (test.shape[0] + batch_size -1) // batch_size\n",
    "\n",
    "        meter_reading = []\n",
    "\n",
    "        for i in tqdm(range(iterations)):\n",
    "            pos = i*batch_size\n",
    "            fold_preds = [np.expm1(model.predict(test.drop(columns=['row_id', 'meter', 'group', \"month\"]).iloc[pos : pos+batch_size], num_iteration=model.best_iteration)) for model in models]\n",
    "            meter_reading.extend(np.mean(fold_preds, axis=0))\n",
    "\n",
    "        submission = pd.DataFrame({\n",
    "            'row_id': test.row_id,\n",
    "            \"meter_reading\": np.clip(meter_reading, a_min=0, a_max=None)\n",
    "        })\n",
    "\n",
    "        save_file = opj(config[\"work_dir\"], \"results\", \"submission_model=%s_meter=%s.csv\" % (config['train']['model'], m))\n",
    "        submission.to_csv(save_file, index=False)\n",
    "\n",
    "    predictions(meter=m, test=test, models=models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_files = [\n",
    "    opj(config[\"work_dir\"], 'results', 'validation_model=lightgbm_meter=0.csv'),\n",
    "    opj(config[\"work_dir\"], 'results', 'validation_model=lightgbm_meter=1.csv'),\n",
    "    opj(config[\"work_dir\"], 'results', 'validation_model=lightgbm_meter=2.csv'),\n",
    "    opj(config[\"work_dir\"], 'results', 'validation_model=lightgbm_meter=3.csv'),\n",
    "]\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for file in prediction_files:\n",
    "    predictions = predictions.append(pd.read_csv(file))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(predictions['meter_reading'], predictions['meter_reading_oof'])))\n",
    "\n",
    "save_file = opj(config[\"work_dir\"], \"results\", \"validation.csv\")\n",
    "predictions.to_csv(save_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.distplot(predictions['meter_reading']).set_title(\"Train-Test Distribution\")\n",
    "sns.distplot(predictions['meter_reading_oof'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_files = [\n",
    "    opj(config[\"work_dir\"], 'results', 'submission_model=lightgbm_meter=0.csv'),\n",
    "    opj(config[\"work_dir\"], 'results', 'submission_model=lightgbm_meter=1.csv'),\n",
    "    opj(config[\"work_dir\"], 'results', 'submission_model=lightgbm_meter=2.csv'),\n",
    "    opj(config[\"work_dir\"], 'results', 'submission_model=lightgbm_meter=3.csv'),\n",
    "]\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "for file in prediction_files:\n",
    "    predictions = predictions.append(pd.read_csv(file))\n",
    "    \n",
    "predictions = predictions.sort_values(by=['row_id'])\n",
    "\n",
    "save_file = opj(config[\"work_dir\"], \"results\", \"submission.csv\")\n",
    "predictions.to_csv(save_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
